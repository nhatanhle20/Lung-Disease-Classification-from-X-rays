{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fa4f4986-d1d5-4f5c-bd68-5a03a5ef3f47",
   "metadata": {},
   "source": [
    "# Install Required Libraries\n",
    "-  The project is implemented using following libraries in Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc720e3d-99b0-4512-98ee-aa378b4917f3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pip install pandas numpy opencv-python matplotlib scikit-learn mahotas tqdm scikit-image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "76a0ec04-504a-4cef-986a-51538f41aa0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import joblib\n",
    "from joblib import parallel_backend\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from skimage.feature import hog, graycomatrix, graycoprops\n",
    "import mahotas\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.decomposition import IncrementalPCA\n",
    "from sklearn.feature_selection import VarianceThreshold"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3ffb067-d038-489a-be06-57b08b493abd",
   "metadata": {},
   "source": [
    "# Read the Data\n",
    "- Load the Metadata CSV file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "03b0a146-dd81-45a6-acf0-70b143d91611",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: (112120, 11)\n",
      "             Image Index          Finding Labels  Follow-up #  Patient ID  \\\n",
      "0       00000001_000.png            Cardiomegaly            0           1   \n",
      "1       00000001_001.png  Cardiomegaly|Emphysema            1           1   \n",
      "2       00000001_002.png   Cardiomegaly|Effusion            2           1   \n",
      "3       00000002_000.png              No Finding            0           2   \n",
      "4       00000003_000.png                  Hernia            0           3   \n",
      "...                  ...                     ...          ...         ...   \n",
      "112115  00030801_001.png          Mass|Pneumonia            1       30801   \n",
      "112116  00030802_000.png              No Finding            0       30802   \n",
      "112117  00030803_000.png              No Finding            0       30803   \n",
      "112118  00030804_000.png              No Finding            0       30804   \n",
      "112119  00030805_000.png              No Finding            0       30805   \n",
      "\n",
      "        Patient Age Patient Gender View Position  OriginalImage[Width  \\\n",
      "0                58              M            PA                 2682   \n",
      "1                58              M            PA                 2894   \n",
      "2                58              M            PA                 2500   \n",
      "3                81              M            PA                 2500   \n",
      "4                81              F            PA                 2582   \n",
      "...             ...            ...           ...                  ...   \n",
      "112115           39              M            PA                 2048   \n",
      "112116           29              M            PA                 2048   \n",
      "112117           42              F            PA                 2048   \n",
      "112118           30              F            PA                 2048   \n",
      "112119           27              M            PA                 2048   \n",
      "\n",
      "        Height]  OriginalImagePixelSpacing[x     y]  \n",
      "0          2749                        0.143  0.143  \n",
      "1          2729                        0.143  0.143  \n",
      "2          2048                        0.168  0.168  \n",
      "3          2048                        0.171  0.171  \n",
      "4          2991                        0.143  0.143  \n",
      "...         ...                          ...    ...  \n",
      "112115     2500                        0.168  0.168  \n",
      "112116     2500                        0.168  0.168  \n",
      "112117     2500                        0.168  0.168  \n",
      "112118     2500                        0.168  0.168  \n",
      "112119     2500                        0.171  0.171  \n",
      "\n",
      "[112120 rows x 11 columns]\n"
     ]
    }
   ],
   "source": [
    "# Path to dataset folder\n",
    "DATASET_PATH = \"dataset\" \n",
    "IMAGE_DIRS = [os.path.join(DATASET_PATH, f\"images_{str(i).zfill(3)}/images\") for i in range(1, 13)]  # images_001 to 12\n",
    "CSV_PATH = os.path.join(DATASET_PATH, \"Data_Entry_2017.csv\")\n",
    "\n",
    "# Load dataset metadata\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "df = df.drop(columns=[\"Unnamed: 11\"], errors=\"ignore\")\n",
    "\n",
    "# Show dataset info\n",
    "print(\"Dataset shape:\", df.shape)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82b7aa3b-178e-4e89-8ed1-11ef95482604",
   "metadata": {},
   "source": [
    "- Dataset Information:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a70c51cb-0e17-482b-98c1-4c32d4be82f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 112120 entries, 0 to 112119\n",
      "Data columns (total 11 columns):\n",
      " #   Column                       Non-Null Count   Dtype  \n",
      "---  ------                       --------------   -----  \n",
      " 0   Image Index                  112120 non-null  object \n",
      " 1   Finding Labels               112120 non-null  object \n",
      " 2   Follow-up #                  112120 non-null  int64  \n",
      " 3   Patient ID                   112120 non-null  int64  \n",
      " 4   Patient Age                  112120 non-null  int64  \n",
      " 5   Patient Gender               112120 non-null  object \n",
      " 6   View Position                112120 non-null  object \n",
      " 7   OriginalImage[Width          112120 non-null  int64  \n",
      " 8   Height]                      112120 non-null  int64  \n",
      " 9   OriginalImagePixelSpacing[x  112120 non-null  float64\n",
      " 10  y]                           112120 non-null  float64\n",
      "dtypes: float64(2), int64(5), object(4)\n",
      "memory usage: 9.4+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45408136-afc6-4928-89b8-cd73801dc120",
   "metadata": {},
   "source": [
    "# Define Labels for Classification:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f369ac0f-a86d-4812-b80e-8b6f78a00e37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Image Index  Binary_Label  Atelectasis  Cardiomegaly  Consolidation  \\\n",
      "0  00000001_000.png             1            0             1              0   \n",
      "1  00000001_001.png             1            0             1              0   \n",
      "2  00000001_002.png             1            0             1              0   \n",
      "3  00000002_000.png             0            0             0              0   \n",
      "4  00000003_000.png             1            0             0              0   \n",
      "\n",
      "   Edema  Effusion  Emphysema  Fibrosis  Hernia  Infiltration  Mass  Nodule  \\\n",
      "0      0         0          0         0       0             0     0       0   \n",
      "1      0         0          1         0       0             0     0       0   \n",
      "2      0         1          0         0       0             0     0       0   \n",
      "3      0         0          0         0       0             0     0       0   \n",
      "4      0         0          0         0       1             0     0       0   \n",
      "\n",
      "   Pleural_Thickening  Pneumonia  Pneumothorax  \n",
      "0                   0          0             0  \n",
      "1                   0          0             0  \n",
      "2                   0          0             0  \n",
      "3                   0          0             0  \n",
      "4                   0          0             0  \n"
     ]
    }
   ],
   "source": [
    "# List of all 14 diseases\n",
    "all_diseases = [\n",
    "    \"Atelectasis\", \"Cardiomegaly\", \"Consolidation\", \"Edema\", \"Effusion\",\n",
    "    \"Emphysema\", \"Fibrosis\", \"Hernia\", \"Infiltration\", \"Mass\", \"Nodule\",\n",
    "    \"Pleural_Thickening\", \"Pneumonia\", \"Pneumothorax\"\n",
    "]\n",
    "\n",
    "# üîπ Binary Classification (Finding vs. No Finding)\n",
    "df[\"Binary_Label\"] = df[\"Finding Labels\"].apply(lambda x: 0 if x == \"No Finding\" else 1)\n",
    "\n",
    "# üîπ Multi-Label Classification\n",
    "for disease in all_diseases:\n",
    "    df[disease] = df[\"Finding Labels\"].apply(lambda x: 1 if disease in x else 0)\n",
    "\n",
    "# Keep only necessary columns\n",
    "df = df[[\"Image Index\", \"Binary_Label\"] + all_diseases]\n",
    "\n",
    "# Show processed data\n",
    "print(df.head())\n",
    "\n",
    "\n",
    "OUTPUT_LABELS_DIR = \"classification_labels\"\n",
    "os.makedirs(OUTPUT_LABELS_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e1d49b2-9cd9-4fa1-81ca-a07cc7f6ed8c",
   "metadata": {},
   "source": [
    "- Save Labels as .npy Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4e2c8cfc-06f1-4e64-a929-09dc73ef829d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Labels saved as binary_labels.npy and multi_labels.npy\n"
     ]
    }
   ],
   "source": [
    "# Save Binary Labels\n",
    "binary_labels = df[\"Binary_Label\"].values  # Extract binary labels as NumPy array\n",
    "np.save(os.path.join(OUTPUT_LABELS_DIR, \"binary_labels.npy\"), binary_labels)\n",
    "\n",
    "# Save Multi-Label Classification Labels\n",
    "multi_labels = df[all_diseases].values  # Extract multi-labels as NumPy array\n",
    "np.save(os.path.join(OUTPUT_LABELS_DIR, \"multi_labels.npy\"), multi_labels)\n",
    "\n",
    "print(\"‚úÖ Labels saved as binary_labels.npy and multi_labels.npy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76e437fb-e1bd-4fc7-bf88-82a91feb36bd",
   "metadata": {},
   "source": [
    "# Load and Preprocess Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3b3dab11-dfe9-4a01-8e59-007bead98619",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Found 'images' folder in dataset\\images_001\\images\n",
      "‚úÖ Found 'images' folder in dataset\\images_002\\images\n",
      "‚úÖ Found 'images' folder in dataset\\images_003\\images\n",
      "‚úÖ Found 'images' folder in dataset\\images_004\\images\n",
      "‚úÖ Found 'images' folder in dataset\\images_005\\images\n",
      "‚úÖ Found 'images' folder in dataset\\images_006\\images\n",
      "‚úÖ Found 'images' folder in dataset\\images_007\\images\n",
      "‚úÖ Found 'images' folder in dataset\\images_008\\images\n",
      "‚úÖ Found 'images' folder in dataset\\images_009\\images\n",
      "‚úÖ Found 'images' folder in dataset\\images_010\\images\n",
      "‚úÖ Found 'images' folder in dataset\\images_011\\images\n",
      "‚úÖ Found 'images' folder in dataset\\images_012\\images\n",
      "‚úÖ Processed 0 images...\n",
      "‚úÖ Processed 5000 images...\n",
      "‚úÖ Processed 10000 images...\n",
      "‚úÖ Processed 15000 images...\n",
      "‚úÖ Processed 20000 images...\n",
      "‚úÖ Processed 25000 images...\n",
      "‚úÖ Processed 30000 images...\n",
      "‚úÖ Processed 35000 images...\n",
      "‚úÖ Processed 40000 images...\n",
      "‚úÖ Processed 45000 images...\n",
      "‚úÖ Processed 50000 images...\n",
      "‚úÖ Processed 55000 images...\n",
      "‚úÖ Processed 60000 images...\n",
      "‚úÖ Processed 65000 images...\n",
      "‚úÖ Processed 70000 images...\n",
      "‚úÖ Processed 75000 images...\n",
      "‚úÖ Processed 80000 images...\n",
      "‚úÖ Processed 85000 images...\n",
      "‚úÖ Processed 90000 images...\n",
      "‚úÖ Processed 95000 images...\n",
      "‚úÖ Processed 100000 images...\n",
      "‚úÖ Processed 105000 images...\n",
      "‚úÖ Processed 110000 images...\n",
      "‚úÖ Resizing complete! Unique images count: 112094\n",
      "‚úÖ Loaded resized images shape: (112120, 12544)\n"
     ]
    }
   ],
   "source": [
    "# Define Image Size and Output Directory\n",
    "DATASET_PATH = \"dataset\" \n",
    "RESIZED_DIR = \"resized_images\"  # Folder to save resized images\n",
    "CSV_PATH = os.path.join(DATASET_PATH, \"Data_Entry_2017.csv\")\n",
    "IMAGE_DIRS = [os.path.join(DATASET_PATH, d) for d in os.listdir(DATASET_PATH) if d.startswith(\"images_\")]\n",
    "IMG_SIZE = (112, 112)  # Resize to 112x112\n",
    "\n",
    "os.makedirs(RESIZED_DIR, exist_ok=True)  # Create folder if not exist\n",
    "\n",
    "# Load metadata CSV\n",
    "df = pd.read_csv(CSV_PATH)  # Update with your metadata file\n",
    "\n",
    "# Function to check folder existence\n",
    "def check_folders():\n",
    "    for folder in IMAGE_DIRS:\n",
    "        images_subdir = os.path.join(folder, \"images\")  # Always use os.path.join()\n",
    "        if not os.path.exists(images_subdir):\n",
    "            print(f\"‚ùå 'images' folder is MISSING in {images_subdir}\")\n",
    "        else:\n",
    "            print(f\"‚úÖ Found 'images' folder in {images_subdir}\")\n",
    "\n",
    "# Run the check\n",
    "check_folders()\n",
    "\n",
    "# Function to find image paths dynamically (inside \"images\" subfolder)\n",
    "def get_image_path(image_index):\n",
    "    \"\"\"\n",
    "    Constructs the image file path dynamically based on the known folder structure.\n",
    "    \"\"\"\n",
    "    for folder in IMAGE_DIRS:  # Iterate over 'images_001' to 'images_012'\n",
    "        img_folder = os.path.join(folder, \"images\")  # Navigate inside \"images\" subfolder\n",
    "        img_path = os.path.join(img_folder, image_index)\n",
    "\n",
    "        if os.path.exists(img_path):\n",
    "            return img_path\n",
    "\n",
    "# Function to resize and save images\n",
    "def resize_and_save_images(df):\n",
    "    unique_images = set()  # Track unique images\n",
    "    \n",
    "    for i, image_index in enumerate(df[\"Image Index\"]):  # Assuming 'Image Index' column exists\n",
    "        img_path = get_image_path(image_index)\n",
    "        if img_path is None:\n",
    "            print(f\"‚ùå Warning: Image {image_index} not found in any folder.\")\n",
    "            continue  # Skip missing images\n",
    "\n",
    "        try:\n",
    "            img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)  # Load as grayscale\n",
    "            if img is None:\n",
    "                print(f\"‚ùå Warning: Could not read {img_path}\")\n",
    "                continue  # Skip if image can't be read\n",
    "\n",
    "            img = cv2.resize(img, IMG_SIZE)  # Resize\n",
    "            save_path = os.path.join(RESIZED_DIR, image_index + \".png\")  # Ensure extension\n",
    "            cv2.imwrite(save_path, img)  # Save image\n",
    "\n",
    "            # Track unique images\n",
    "            unique_images.add(img.tobytes())\n",
    "\n",
    "            if i % 5000 == 0:\n",
    "                print(f\" Processed {i} images...\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Error processing {img_path}: {e}\")\n",
    "\n",
    "    print(f\"‚úÖ Resizing complete! Unique images count: {len(unique_images)}\")\n",
    "\n",
    "# Function to load resized images\n",
    "def load_resized_images(image_folder):\n",
    "    images = []\n",
    "    filenames = sorted(os.listdir(image_folder))  # Ensure order consistency\n",
    "\n",
    "    for filename in filenames:\n",
    "        img_path = os.path.join(image_folder, filename)\n",
    "        img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)  # Load resized image\n",
    "        \n",
    "        if img is None:\n",
    "            print(f\"‚ùå Error: Could not load {img_path}\")\n",
    "            continue\n",
    "\n",
    "        img = img / 255.0  # Normalize pixel values\n",
    "        images.append(img.flatten())  # Flatten for ML\n",
    "\n",
    "    return np.array(images)\n",
    "\n",
    "# Run the resizing function\n",
    "resize_and_save_images(df)\n",
    "\n",
    "# Load resized images\n",
    "X = load_resized_images(RESIZED_DIR)\n",
    "print(\"‚úÖ Loaded resized images shape:\", X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8081c9c6-be9b-45fe-88b3-e027311cfab3",
   "metadata": {},
   "source": [
    "# Apply PCA for Dimensionality Reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ff8a2640-ca35-42c1-a0af-64046098f1bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Fitting PCA incrementally on image batches...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 23/23 [47:36<00:00, 124.19s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ PCA fitting completed.\n",
      "üîÑ Transforming images using trained PCA...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Transforming Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 23/23 [01:19<00:00,  3.44s/batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ PCA transformation completed and saved as 'pca_features.npy'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ‚úÖ Define Directories & Parameters\n",
    "RESIZED_DIR = \"resized_images\"  # Directory containing resized images\n",
    "BATCH_SIZE = 5000  # Adjust based on RAM size\n",
    "N_COMPONENTS = 100  # Adjust based on experiments\n",
    "\n",
    "# Get sorted filenames\n",
    "image_files = sorted(os.listdir(RESIZED_DIR))\n",
    "\n",
    "# Define PCA model\n",
    "pca = IncrementalPCA(n_components=N_COMPONENTS, batch_size=BATCH_SIZE)\n",
    "\n",
    "# ‚úÖ First Pass: Fit PCA on Batches\n",
    "print(\"Fitting PCA incrementally on image batches...\")\n",
    "\n",
    "for i in tqdm(range(0, len(image_files), BATCH_SIZE), desc=\"Processing Batches\", unit=\"batch\"):\n",
    "    batch_images = []\n",
    "\n",
    "    for filename in image_files[i:i + BATCH_SIZE]:\n",
    "        img_path = os.path.join(RESIZED_DIR, filename)\n",
    "        img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "        # ‚úÖ Fix: Ensure the image is loaded correctly\n",
    "        if img is None:\n",
    "            print(f\"‚ö†Ô∏è Warning: Could not read {filename}, skipping.\")\n",
    "            continue\n",
    "\n",
    "        img = img / 255.0  # Normalize\n",
    "        img_flattened = img.flatten()\n",
    "\n",
    "        # ‚úÖ Fix: Skip images that are all zeros (empty)\n",
    "        if np.all(img_flattened == 0):\n",
    "            print(f\"‚ö†Ô∏è Warning: {filename} is all zeros, skipping.\")\n",
    "            continue\n",
    "\n",
    "        batch_images.append(img_flattened)\n",
    "\n",
    "    # ‚úÖ Fix: Ensure batch is not empty before calling PCA\n",
    "    if len(batch_images) == 0:\n",
    "        print(f\"‚ö†Ô∏è Skipping empty batch {i // BATCH_SIZE + 1}.\")\n",
    "        continue\n",
    "\n",
    "    batch_images = np.array(batch_images)\n",
    "    \n",
    "    # Fit PCA incrementally\n",
    "    pca.partial_fit(batch_images)\n",
    "\n",
    "print(\"‚úÖ PCA fitting completed.\")\n",
    "\n",
    "# ‚úÖ Second Pass: Transform Data using PCA\n",
    "X_pca = []\n",
    "print(\"Transforming images using trained PCA...\")\n",
    "\n",
    "for i in tqdm(range(0, len(image_files), BATCH_SIZE), desc=\"Transforming Batches\", unit=\"batch\"):\n",
    "    batch_images = []\n",
    "\n",
    "    for filename in image_files[i:i + BATCH_SIZE]:\n",
    "        img_path = os.path.join(RESIZED_DIR, filename)\n",
    "        img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "        if img is None:\n",
    "            continue\n",
    "\n",
    "        img = img / 255.0\n",
    "        img_flattened = img.flatten()\n",
    "\n",
    "        if np.all(img_flattened == 0):\n",
    "            continue\n",
    "\n",
    "        batch_images.append(img_flattened)\n",
    "\n",
    "    if len(batch_images) == 0:\n",
    "        continue\n",
    "\n",
    "    batch_images = np.array(batch_images)\n",
    "    X_pca.append(pca.transform(batch_images))\n",
    "\n",
    "# ‚úÖ Convert to Array & Save\n",
    "X_pca = np.vstack(X_pca)\n",
    "X_pca = np.array(X_pca, dtype=np.float16)  # Reduce storage size\n",
    "np.save(\"pca_features.npy\", X_pca)\n",
    "\n",
    "print(\"‚úÖ PCA transformation completed and saved as 'pca_features.npy'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e88b1660-b078-4645-bb62-440aca18d12c",
   "metadata": {},
   "source": [
    "# Train Binary Classification Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e00fe554-841e-4dd9-96bc-07118d0aecc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded PCA Features Shape: (112120, 100)\n",
      "Labels Shape: (112120,)\n",
      "\n",
      " Dataset Information:\n",
      "Total Samples: 112120\n",
      "Feature Dimensions: 100\n",
      "Label Distribution:\n",
      "[60361 51759]\n",
      "\n",
      " Data Split:\n",
      "Training Samples: 89696\n",
      "Test Samples: 22424\n",
      "\n",
      " Training SVM Classifier...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [1:30:53<00:00, 5453.68s/model]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Model Evaluation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluation Progress: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [02:31<00:00, 151.31s/eval]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Model Accuracy: 0.6593\n",
      "\n",
      " Detailed Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.75      0.70     12072\n",
      "           1       0.66      0.55      0.60     10352\n",
      "\n",
      "    accuracy                           0.66     22424\n",
      "   macro avg       0.66      0.65      0.65     22424\n",
      "weighted avg       0.66      0.66      0.66     22424\n",
      "\n",
      "\n",
      "‚úÖ Model saved as 'svm_binary_model.pkl'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Define file paths\n",
    "PCA_FEATURES_FILE = \"pca_features.npy\"  # Single file instead of batches\n",
    "LABEL_PATH = \"classification_labels\" \n",
    "BINARY_LABELS_FILE = os.path.join(LABEL_PATH, \"binary_labels.npy\")\n",
    "\n",
    "def train_svm_binary_classifier(\n",
    "    X_pca, \n",
    "    y, \n",
    "    test_size=0.2, \n",
    "    random_state=42, \n",
    "    kernel='rbf', \n",
    "    C=1.0\n",
    "):\n",
    "    \"\"\"\n",
    "    Train an SVM Binary Classifier with progress tracking.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    X_pca : numpy.ndarray\n",
    "        PCA-transformed feature matrix\n",
    "    y : numpy.ndarray\n",
    "        Binary labels\n",
    "    test_size : float, optional (default=0.2)\n",
    "        Proportion of the dataset to include in the test split\n",
    "    random_state : int, optional (default=42)\n",
    "        Controls the shuffling applied to the data before splitting\n",
    "    kernel : str, optional (default='rbf')\n",
    "        Specifies the kernel type to be used in the algorithm\n",
    "    C : float, optional (default=1.0)\n",
    "        Regularization parameter\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    tuple: (trained_svm_model, training_metrics)\n",
    "    \"\"\"\n",
    "    # Print initial data information\n",
    "    print(\"\\n Dataset Information:\")\n",
    "    print(f\"Total Samples: {X_pca.shape[0]}\")\n",
    "    print(f\"Feature Dimensions: {X_pca.shape[1]}\")\n",
    "    print(f\"Label Distribution:\\n{np.bincount(y)}\")\n",
    "    \n",
    "    # Stratified train-test split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X_pca, y, \n",
    "        test_size=test_size, \n",
    "        random_state=random_state, \n",
    "        stratify=y\n",
    "    )\n",
    "    \n",
    "    # Print split information\n",
    "    print(\"\\n Data Split:\")\n",
    "    print(f\"Training Samples: {X_train.shape[0]}\")\n",
    "    print(f\"Test Samples: {X_test.shape[0]}\")\n",
    "    \n",
    "    # Initialize SVM Classifier\n",
    "    svm_model = SVC(\n",
    "        kernel=kernel, \n",
    "        C=C, \n",
    "        random_state=random_state,\n",
    "        probability=True  # Enable probability estimates\n",
    "    )\n",
    "    \n",
    "    # Training with progress bar\n",
    "    print(\"\\n Training SVM Classifier...\")\n",
    "    with tqdm(total=1, desc=\"Training Progress\", unit=\"model\") as pbar:\n",
    "        svm_model.fit(X_train, y_train)\n",
    "        pbar.update(1)\n",
    "    \n",
    "    # Evaluation with progress bar\n",
    "    print(\"\\n Model Evaluation...\")\n",
    "    with tqdm(total=1, desc=\"Evaluation Progress\", unit=\"eval\") as pbar:\n",
    "        y_pred = svm_model.predict(X_test)\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        pbar.update(1)\n",
    "    \n",
    "    # Print evaluation metrics\n",
    "    print(f\"\\n‚úÖ Model Accuracy: {accuracy:.4f}\")\n",
    "    print(\"\\n Detailed Classification Report:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    \n",
    "    # Prepare training metrics\n",
    "    training_metrics = {\n",
    "        'accuracy': accuracy,\n",
    "        'total_samples': X_pca.shape[0],\n",
    "        'training_samples': X_train.shape[0],\n",
    "        'test_samples': X_test.shape[0],\n",
    "        'kernel': kernel,\n",
    "        'C': C\n",
    "    }\n",
    "    \n",
    "    return svm_model, training_metrics\n",
    "    \n",
    "# Load PCA-transformed features\n",
    "X_pca = np.load(PCA_FEATURES_FILE)\n",
    "print(\"Loaded PCA Features Shape:\", X_pca.shape)\n",
    "    \n",
    "# Load binary labels (Finding vs. No Finding)\n",
    "y = np.load(BINARY_LABELS_FILE)\n",
    "print(\"Labels Shape:\", y.shape)\n",
    "    \n",
    "# Ensure feature and label count match\n",
    "assert X_pca.shape[0] == y.shape[0], \"Error: Features and labels count mismatch!\"\n",
    "    \n",
    "# Train SVM Classifier\n",
    "svm_model, metrics = train_svm_binary_classifier(X_pca, y)\n",
    "    \n",
    "# Save Trained Model\n",
    "joblib.dump(svm_model, \"svm_binary_model.pkl\")\n",
    "print(\"\\n‚úÖ Model saved as 'svm_binary_model.pkl'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f4629ff-f539-4dcd-98fb-ca772b92e882",
   "metadata": {},
   "source": [
    "# Train Multi-Label Classification Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bed36728-4be7-4608-a36b-c3839769f7d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and processing images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Images: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 112120/112120 [09:51<00:00, 189.59img/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded Image Data Shape: (112120, 12544)\n",
      "Labels Shape: (112120, 14)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Multi-Output Random Forest: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [38:46<00:00, 2326.68s/model]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Training Completed Successfully!\n",
      "\n",
      "üîç Training Details:\n",
      "- Number of estimators: 50\n",
      "- Training data shape: (89696, 12544)\n",
      "- Number of output labels: 14\n",
      "‚úÖ Model Accuracy: 0.5394\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.21      0.00      0.01      2225\n",
      "           1       1.00      0.00      0.00       557\n",
      "           2       0.00      0.00      0.00       894\n",
      "           3       1.00      0.00      0.00       446\n",
      "           4       0.44      0.02      0.04      2625\n",
      "           5       0.00      0.00      0.00       487\n",
      "           6       0.00      0.00      0.00       319\n",
      "           7       1.00      0.00      0.00        46\n",
      "           8       0.39      0.02      0.03      3951\n",
      "           9       0.18      0.00      0.00      1141\n",
      "          10       0.00      0.00      0.00      1285\n",
      "          11       0.00      0.00      0.00       663\n",
      "          12       0.00      0.00      0.00       294\n",
      "          13       0.17      0.00      0.00      1046\n",
      "\n",
      "   micro avg       0.35      0.01      0.02     15979\n",
      "   macro avg       0.31      0.00      0.01     15979\n",
      "weighted avg       0.29      0.01      0.02     15979\n",
      " samples avg       0.99      0.55      0.54     15979\n",
      "\n",
      "Label distribution in training data:\n",
      "[ 9334  2219  3773  1857 10692  2029  1367   181 15943  4641  5046  2722\n",
      "  1137  4256]\n",
      "\n",
      "Label distribution in test data:\n",
      "[2225  557  894  446 2625  487  319   46 3951 1141 1285  663  294 1046]\n",
      "‚úÖ Multi-Label Model saved as 'random_forest_multi_label.pkl'.\n"
     ]
    }
   ],
   "source": [
    "# Directories\n",
    "IMAGE_DIR = \"resized_images\"  # Directory containing resized images\n",
    "LABEL_PATH = \"classification_labels\"\n",
    "MULTILABEL_FILE = os.path.join(LABEL_PATH, \"multi_labels.npy\")\n",
    "\n",
    "# Image Preprocessing Parameters\n",
    "IMAGE_SIZE = (112, 112)  # Ensure this matches your resizing process\n",
    "\n",
    "# Load all images as flattened feature vectors\n",
    "image_files = sorted(os.listdir(IMAGE_DIR))  # Ensure order matches labels\n",
    "X_images = []\n",
    "\n",
    "print(\"Loading and processing images...\")\n",
    "for img_file in tqdm(image_files, desc=\"Processing Images\", unit=\"img\"):\n",
    "    img_path = os.path.join(IMAGE_DIR, img_file)\n",
    "    img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)  # Load as grayscale\n",
    "    img = cv2.resize(img, IMAGE_SIZE)  # Resize if needed\n",
    "    X_images.append(img.flatten())  # Flatten into 1D vector\n",
    "\n",
    "# Convert to NumPy array\n",
    "X_images = np.array(X_images, dtype=np.float32) / 255.0  # Normalize pixel values\n",
    "print(\"Loaded Image Data Shape:\", X_images.shape)\n",
    "\n",
    "# Load multi-label classification labels\n",
    "y_multi = np.load(MULTILABEL_FILE)  # Ensure labels match image count\n",
    "print(\"Labels Shape:\", y_multi.shape)\n",
    "\n",
    "# Ensure feature and label count match\n",
    "assert X_images.shape[0] == y_multi.shape[0], \"Error: Image count and labels count mismatch!\"\n",
    "\n",
    "# Debug Function\n",
    "# print(f\"Number of images: {len(image_files)}, Number of labels: {len(y_multi)}\")\n",
    "# print(\"Label Distribution:\\n\", np.sum(y_multi, axis=0))  # Sum labels per class\n",
    "# print(\"Unique values in labels:\", np.unique(y_multi))\n",
    "# print(\"Mean pixel values per image:\", np.mean(X_images, axis=1)[:10])  # Check first 10 images\n",
    "# print(\"Standard deviation per image:\", np.std(X_images, axis=1)[:10])\n",
    "# print(\"Are all images identical?\", np.all(X_images[0] == X_images))\n",
    "\n",
    "\n",
    "# Split Data: 80% Train, 20% Test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_images, y_multi, test_size=0.2, random_state=42)\n",
    "\n",
    "def train_multi_output_random_forest(X_train, y_train, n_estimators=50, random_state=42):\n",
    "    \"\"\"\n",
    "    Train a Multi-Output Random Forest Classifier with a progress bar.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    X_train : array-like of shape (n_samples, n_features)\n",
    "        Training data features\n",
    "    y_train : array-like of shape (n_samples, n_outputs)\n",
    "        Training data multi-label targets\n",
    "    n_estimators : int, optional (default=50)\n",
    "        Number of trees in the forest\n",
    "    random_state : int, optional (default=42)\n",
    "        Controls randomness of the model\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    MultiOutputClassifier: Trained multi-output random forest model\n",
    "    \"\"\"\n",
    "    # Create the multi-output classifier\n",
    "    rf_model = MultiOutputClassifier(\n",
    "        RandomForestClassifier(\n",
    "            n_estimators=n_estimators, \n",
    "            random_state=random_state,\n",
    "            n_jobs=-1  # Use all available cores\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    # Wrap the fitting process with tqdm\n",
    "    with tqdm(total=1, desc=\"Training Multi-Output Random Forest\", unit=\"model\") as pbar:\n",
    "        rf_model.fit(X_train, y_train)\n",
    "        pbar.update(1)\n",
    "    \n",
    "    print(\"\\n‚úÖ Training Completed Successfully!\")\n",
    "    \n",
    "    # Print additional training information\n",
    "    print(\"\\nüîç Training Details:\")\n",
    "    print(f\"- Number of estimators: {n_estimators}\")\n",
    "    print(f\"- Training data shape: {X_train.shape}\")\n",
    "    print(f\"- Number of output labels: {y_train.shape[1]}\")\n",
    "    \n",
    "    return rf_model\n",
    "\n",
    "rf_model = train_multi_output_random_forest(X_train, y_train)\n",
    "\n",
    "# Evaluate Model\n",
    "y_pred = rf_model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"‚úÖ Model Accuracy: {accuracy:.4f}\")\n",
    "print(classification_report(y_test, y_pred, zero_division=1))\n",
    "\n",
    "# Check label distribution\n",
    "print(\"Label distribution in training data:\")\n",
    "print(np.sum(y_train, axis=0))\n",
    "print(\"\\nLabel distribution in test data:\")\n",
    "print(np.sum(y_test, axis=0))\n",
    "\n",
    "# Save Trained Model\n",
    "joblib.dump(rf_model, \"random_forest_multi_label.pkl\")\n",
    "print(\"‚úÖ Multi-Label Model saved as 'random_forest_multi_label.pkl'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f319b92-ecaa-49f5-b5c0-10d6ce5a4287",
   "metadata": {},
   "source": [
    "# Load and Use Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1db4bd96-5ee3-4805-bf25-b98dae5bdaf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load PCA model\n",
    "pca_loaded = joblib.load(\"pca_model.pkl\")\n",
    "\n",
    "# Load binary classification SVM model\n",
    "svm_loaded = joblib.load(\"svm_model.pkl\")\n",
    "\n",
    "# Load multi-label classification model\n",
    "rf_loaded = joblib.load(\"random_forest_multi_label.pkl\")\n",
    "\n",
    "# Predict on a new sample (assume X_new_features is preprocessed)\n",
    "y_new_pred = rf_model.predict(X_new_features)\n",
    "print(\"Predicted Labels:\", y_new_pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
